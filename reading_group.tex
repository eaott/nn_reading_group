\documentclass{article}
\usepackage[top=.5in, bottom=.5in, left=.9in, right=.9in]{geometry}
\usepackage[latin1]{inputenc}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tabularx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{siunitx}
\usepackage{mathtools}
\usepackage{bbm}




% Use instead of natbib
\usepackage[backend=bibtex,citestyle=authoryear-comp,natbib=true,sorting=none,hyperref=true,maxnames=4,arxiv=pdf]{biblatex}
\renewbibmacro{in:}{}
\addbibresource{/Users/Evan/GitProjects/biomedical/references.bib}




\newcommand{\obar}[1]{\ensuremath{\overline{ #1 }}}
\newcommand{\iid}{\ensuremath{\stackrel{\textrm{iid}}{\sim}}}
\newcommand{\op}[2]{{\ensuremath{\underset{ #2 }{\operatorname{ #1 }}~}}}
\newcommand{\norm}[1]{{ \ensuremath{ \left\lVert  #1 \right\rVert  }  }}
\newcommand{\cov}{ \ensuremath{ \textrm{cov} } }
\newcommand{\var}{ \ensuremath{ \textrm{var} } }
\newcommand{\tr}{ \ensuremath{ \textrm{trace} } }
\newcommand{\df}{ \ensuremath{ \textrm{df} } }
\newcommand{\E}{ \ensuremath{ \mathbb{E} }}
\newcommand{\V}{ \ensuremath{ \mathbb{V} }}
\newcommand{\R}{ \ensuremath{ \mathbb{R} }}
\newcommand{\indicator}[1]{ \ensuremath{ \mathbbm{1}\left\{ #1 \right\} }   }

\usepackage{xcolor}
\definecolor{darkgreen}{rgb}{0,0.25,0}
\newcommand{\soln}{{\color{red}\textbf{Solution:~}\color{black}}}


\usepackage[formats]{listings}
\lstdefineformat{R}{~={\( \sim \)}}
\lstset{% general command to set parameter(s)
basicstyle=\small\ttfamily, % print whole listing small
keywordstyle=\bfseries\rmfamily,
keepspaces=true,
% underlined bold black keywords
commentstyle=\color{darkgreen}, % white comments
stringstyle=\ttfamily, % typewriter type for strings
showstringspaces=false,
numbers=left, numberstyle=\tiny, stepnumber=1, numbersep=5pt, %
frame=shadowbox,
rulesepcolor=\color{black},
,columns=fullflexible,format=R
} %
\renewcommand{\ttdefault}{cmtt}
% enumerate is numbered \begin{enumerate}[(I)] is cap roman in parens
% itemize is bulleted \begin{itemize}
% subfigures:
% \begin{subfigure}[b]{0.5\textwidth} \includegraphics{asdf.jpg} \caption{} \label{subfig:asdf} \end{subfigure}
\hypersetup{colorlinks=true, urlcolor=blue, linkcolor=blue, citecolor=blue}


\graphicspath{ {C:/Users/Evan/Desktop/} }
\title{\vspace{-6ex}NN/VI/Autoencoder Reading Group Notes\vspace{-2ex}}
\author{Evan Ott \\ \href{mailto:evan.ott@utexas.edu}{evan.ott@utexas.edu}\vspace{-2ex}}
\date{Last updated: \today}
\setcounter{secnumdepth}{0}

\usepackage[parfill]{parskip}

\newcommand{\mycite}[1]{\item[\cite{#1}] ~\\ \fullcite{#1} \\ ~ \\}

\begin{document}
\maketitle

\section{Outline}
\begin{itemize}
  \item Variational inference (James)
  \item Stochastic variational inference (Michael)
  \item NN and backpropagation (Yuguang)
  \item Introduction to TensorFlow (Mo and Evan) -- actual hello world (like a single layer perceptron or a linear model) -- leave out complicated things like dropout, etc.
  \item Classical autoencoders (Mauricio)
  \item Variational autoencoders (Jennifer)
\end{itemize}

We'll have a GitHub page / Google Doc for papers, examples, talks, code, etc.

That's at \url{https://github.com/jestarling/DeepProbModels}

\section{2017-09-11: Variational Inference (James)}
\subsection{``Variational Bayes for Idiots''}
Today is a basic introduction to variational inference.

Observed data $X$ and hidden variables $Z$ (could be parameters for the whole dataset
-- like means and variances of mixture components of a GMM -- or the per-data-point hidden variable -- like an indicator
for each datapoint being in a cluster).

\begin{align*}
  p(x,z)&= p(x|z)\cdot p(z)
  \\
  \textrm{Bayes:~} p(z|x)&=\frac{p(x|z)p(z)}{p(x)}
  \\
  \textrm{\color{red}Problem:~\color{black}} p(x)=\int_Z p(x,z)dz
\end{align*}
$p(x)$ is challenging or impossible to compute.

Idea:
\begin{itemize}
  \item posit some family of approximations, $\mathcal{Q}$
  \item Find the member of $\mathcal{Q}$ that is ``closest'' to $p(z|x)$ in KL-divergence (or other measure)

  \hspace{5ex} Formally:
  \begin{equation}
    q^*(z)=\arg\min_{q(z)\in\mathcal{Q}}KL \left(q(z) \middle\| p(z|x)\right)
  \end{equation}
  where
  \begin{align}
    \label{eq:KL}
    KL \left(q(z) \middle\| p(z|x)\right) &= \E_q(z)\left[\log\frac{q(z)}{p(z|x)}\right]
    \\
    &= \E_q\left[\log q(z)\right] - \E_q\left[\log p(z|x)\right] \notag
  \end{align}
\end{itemize}

This is a calculus of variations problem (like the brachistochrome problem), where this will eventually
lead to a vector optimization problem that's standing for a functional optimization problem.

This also will give us point estimates that are often better than running a Gibbs sampler, and be faster.
KL will favor putting probability mass where $p(z|x)$ is large, so that the variance of the approximated
distribution is often smaller than the variance of the true posterior. If we flipped the ratio in equation
\ref{eq:KL}, we would instead be looking at Expectation Propagation.

\subsubsection{Example: One-Hot Encoding}
\begin{align*}
  (x_i|\mu, c_i) &\sim N(c_i^\top \mu, 1)
  \\
  \textrm{e.g.,~}c_i &= (0, 0, 1, 0, 0) \in \{0,1\}^k
  \\
  \mu\in\mathbb{R}^k
  \\
  \mu_k&\sim N(0,\tau^2)
\end{align*}
Joint of data, parameters:
\begin{equation*}
  p(\mu,c,x)=p(\mu)\prod_{i=1}^N p(c_i)p(x_i|\mu,c_i)
\end{equation*}
Marginal or ``Evidence'': $p(x)=\int_{\mu,c}p(x,\mu,c)p(\mu,c)d\mu dc$

\subsubsection{ELBO}
ELBO: Evidence lower bound.

$\mathcal{Q}$: family of approximations.

Each $q(z)\in\mathcal{Q}$ is a candidate.

$\arg\min_{q(z)\in\mathcal{Q}}KL \left(q(z) \middle\| p(z|x)\right)$ is not computable:

\begin{align*}
  \E_q\left[\log q(z)\right] - \E_q\left[\log p(z|x)\right] &= \E_q\left[\log q(z)\right] - \E_q\left[\log p(x,z) - \log p(x)\right]
\end{align*}
But we can't compute $p(x)$, so we can't compute it, but the term is not dependent on the choice of $z$. So, we ignore that term,
and want to maximize its negative (the ELBO):
\begin{align*}
  ELBO(q)&=\E_q\left[\log p(x,z)\right]-\E_q\left[\log q(z)\right]
  \\
  &=-KL \left(q(z) \middle\| p(z|x)\right) + \log(p(x))
  \\
  &\leq \log(p(x))
\end{align*}
So we can re-write this as:
\begin{align*}
  ELBO(q)&=\E_q\left[\log p(x|z)\right]+\E_q\left[\log p(z)\right]-\E_q\left[\log q(z)\right]
  \\
  &=\E_q\left[\log p(x|z)\right]-KL\left(q(z)\middle\|p(z)\right)
\end{align*}
So we're making the likelihood as large as possible, and making the
approximate distribution that penalizes moving away from the prior. In other words,
be close to the data, and don't stray far from the prior.

\subsection{Note}
Carlos asked a question about not having to do the expectation of the log posterior with respect to $q$ to get
something more like a MAP estimate. But a reasonable compromise is doing a maximum marginal a posteriori estimate
where we marginalize over the local variables in the model (the cluster assignments in a GLM), but keep the global
parameters to find the MAP. That's a nice sort of Bayesian thing to do.

\subsection{Mean-field family}
This is a family of approximations where the correlation structure is completely independent.

\begin{equation*}
  \mathcal{Q}=\left\{q(z) : q(z)=\prod_{j=1}^M q_j(z_j)\right\}
\end{equation*}
This is not a model of the data (there's no $x$ in that). But we're going to connect it to the data
through the ELBO. Now, we have options for each $q_j$, where we might take a nice parametric form. We might have Gaussians for location parameters
or inverse gammas for scale parameters. But for some models where the complete conditionals are exponential, we can find optimal approximation to take for each $q_j$.

\subsection{Coordinate ascent}
Not totally different from Gibbs sampling, only we're doing optimization instead of probabilistic draws. We'll do
one latent variable at a time, such as all the components one after another, then each mean in a GMM. Then, start over.

Consider the $j$th latent variable $z_j$ with complete conditional $p(z_j|z_{-j}, x)$. With $z_{-j}$ fixed,
the optimal $q_j$ (that makes ELBO as large as possible) is of the form:

\begin{equation}
  q_j^*(z_j)\propto \exp\left\{ \E_{-j}\left[ \log p(z_j | z_{-j}, x)\right] \right\}
\end{equation}
Or, can write in terms of the joint (the constants will go away). The expectation is with
respect to the variational density over all the other parameters, i.e.,
\begin{equation*}
  z_{-j}\sim \prod_{l\neq j} q_l(z_l)
\end{equation*}

So, we set $q_j^*(z_j)$ for each $j$, cycle through, update, etc. until it converges.

\section{2017-09-18: Variational Inference (James)}

\subsection{CAVI}
Pick up from last time on Coordinate Ascent Variational Inference (CAVI).

Consider $z_j$. Complete conditional is $p(z_j | z_{-j}, x)$. Fact: with $z_{-j}$ fixed, the optimal $q_j(z_j)$
is of the form
\begin{equation*}
  \log q_j^*(z_j) =\E_{-j} \log(p(z_j | z_{-j, x}))
\end{equation*}
where $\E_{-j}$ refers to taking the expectation over $q_{-j}(z_{-j})=\prod_{l\neq j} q_l(z_l)$.

So to do CAVI, cycle through $j=1,\ldots, m$, each time updating $q_j(z_j)=q_j^*(z_j)$ until no improvements are made
to the ELBO.

\subsubsection{Proof of $q_j^*$}
\begin{align*}
  ELBO(q)=\E_q[\log p(x,z)]-\E_q[\log q(z)]
\end{align*}
We want to isolate $q_j(z_j)$:
\begin{align*}
  ELBO(q_j)&=\E_{q_j} \left[ \E_{q_{-j}} \left[ \log p(x, z_j, z_{-j}) \right]\right] -
  \E_{q_j}\left[ \log q_j(z_j) \right] + \textrm{ constant}
  \\
  &=\E_{q_j}\left[ \log q_j^*(z_j) \right] - \E_{q_j}\left[ \log q_j(z_j) \right]
  \\
  &=-KL \left( q_j \| q_j^* \right) - \E_{q_j}\left[ \log q_j(z_j) \right]
\end{align*}
Which guarantees that $q_j^*$ is optimal, because we are minimizing the KL divergence between $q_j$ and $q_j^*$.

\subsection{GMM}
\begin{align*}
  x_i&\in\R~~~~i=1,\ldots,N
  \\
  \mu_k:&k^{\textrm{th}}\textrm{ mean}~~~~k=1,\ldots,L
  \\
  c_i:& \textrm{ indicator vector (``one hot encoding'')}
  \\
  x_i | \mu &\sim N(c_i^\top \mu, 1)
  \\
  \mu_k &\sim N(0, \tau^2)
\end{align*}

For now, set $\tau$ as fixed, to focus on inference for $\mu$ and $c$.

Mean-field family:
\begin{align*}
  q(z)&=\prod_{j}q_j(z_j)
  \\
  &=\prod_{l=1}^L q_k(\mu_k) \cdot \prod_{i=1}^N q_i(c_i)
  \\
  &=\prod_{l=1}^L N(\mu_k| m_k, s_k^2) \cdot \prod_{i=1}^N \textrm{Multinomial}(c_i | \phi_i)
\end{align*}

Now, go through the ELBO, not really different from having to derive a Gibbs sampler.

\begin{align*}
  ELBO(q) &=
    \E_q \left[ \log \left( \prod_{i=1}^N N(x_i|c_i^\top \mu, 1) \cdot p(\mu, c) \right) \right]
    - \E_q \left[ \log \left( \prod_{l=1}^L q(\mu_k| m_k, s_k^2) \right) + \log \left( \prod_{i=1}^N q(c_i | \phi_i) \right)\right]
  \\
  &= \sum_{k=1}^L \E_q \left[ \log p(\mu_k) ; m_k, s_k^2 \right]
  + \sum_{i=1}^N \E_q \left[ \log p(c_i) ; \phi_i \right]
  + \sum_{i=1}^N \E_q \left[ \log p(x_i | c_i, \mu) ; \phi_i, m, s^2 \right]
  - \sum_{i=1}^N \E_q \left[ \log q_i(c_i; \phi_i) \right]
  - \sum_{k=1}^L \E_q \left[ \log q_k(\mu_k; m_k, s_k^2) \right]
\end{align*}

So the update for $c_i$ is
\begin{align*}
  q^*(c_i) &= \log p(c_i) + \E_{-c_i} \left[ \log p(x_i | c_i, \mu) ; m, s^2 \right]
  \\
  \log p(c_i) &= \log \left( \frac{1}{K} \right) = \textrm{ constant in } c_i
  \\
  \E \left[ \log p(x_i | c_i, \mu) ; m, s^2 \right] &= \E \left[ -\frac12 \sum_{k=1}^L c_{ik}(x_i-\mu_k)^2 ; m, s^2 \right]
  \\
  &= \E \left[ -\frac12 \sum_{k=1}^L c_{ik}(x_i^2-2x_i\mu_k+\mu_k^2) ; m, s^2 \right]
  \\
  &= \sum_{k=1}^L \left[  \E \left( -\frac12  c_{ik} x_i^2 \right) + \E \left( c_{ik}\mu_k \right)x_i - \frac12 \E(c_{ik}\mu_k^2) \right]
  \\
  &= \textrm{const} + \sum_{k=1}^L \left[ c_{ik} x_i \E[\mu_k] - \frac12 c_{ik} \E[\mu_k^2] \right]
  \\
  &= \sum_{k=1}^L c_{ik} \left[ x_i \E[\mu_k] - \frac12 \E[\mu_k^2] \right]
  \\
  \E[\mu_k] &= m_k
  \\
  \E[\mu_k^2] &= s_k^2 + m_k^2
  \\
  q^*(c_i| \phi_i) &\propto \prod_{k=1}^L (e^{\psi_{ik}})^{c_{ik}}
  \\
  \psi_{ik} &= x_i m_k - \frac12 (s_k^2 + m_k^2)
\end{align*}
\emph{The above was not edited and may contain errors or omissions.}

In the GitHub repo, see the Variational Inference section.

\section{2017-09-25: Stochastic Variational Inference (Michael)}
Basic idea: instead of using the full dataset, use samples.
In general, the posterior is hard. Want to minimize KL divergence, or
maximize ELBO.

Today, working with GMM again. Make the mean-field approximation, and have it factor
nicely. Then, get exponential family:
\begin{equation*}
  P(X|\theta)=h(X)\exp \left[ \sum_i \eta_i(\theta) T_i(X)-A(\theta) \right]
\end{equation*}
Where $\eta(\theta)$ is the ``natural parameter.'' If complete conditional and variational
approximation forms are both in the same exponential family, everything is easy. Below, $\lambda$ will be variational
for $\beta$ and $\phi_n$ will be variational for $z$, the indicators.

Typically:
\begin{equation*}
  \lambda^{(t)}=\lambda^{(t-1)}+\rho\nabla f(\lambda)
\end{equation*}

But instead, move with the ``natural gradient'' (rather than the gradient as-is). A motivating example is looking at
pairs of normal distributions: $N(0, 10000)$ and $N(10, 10000)$ are obviously close together, whereas $N(0, 0.1)$ and $N(10, 0.1)$
are obviously far apart. The natural gradient moves in directions that are roughly constant length in ``information space''
rather than Euclidean space. It's not altogether different from comparing the gradient and the Newton direction.

The natural gradient is the inverse Fisher information matrix times the standard gradient (notation from \url{http://andymiller.github.io/2016/10/02/natural_gradient_bbvi.html}):
\begin{align*}
  F_\lambda &= \mathbb{E}_{q(\cdot; \lambda)} \left[ \nabla_\lambda \ln q(x; \lambda) \left( \nabla_\lambda \ln q(x; \lambda) \right)^\intercal \right]
  \\
  \tilde \nabla_\lambda &= F_\lambda^{-1} \nabla_\lambda \mathcal{L}(\lambda)
\end{align*}

Gradient: Pick a coordinate direction
and maximize the difference between where you are and where you're going with a particular Euclidean step size.

Natural gradient: do the same thing, but with a particular KL-divergence step size.

However, the natural gradient for exponential family is much easier:
\begin{align*}
  \hat{\nabla} f(\lambda) &= \E_Q \left[ \eta(\cdot) \right] - \lambda
  \\
  \lambda^{(t)}&=(1-\rho)\lambda^{(t-1)}+\rho \E_Q \left[ \eta(\cdot) \right]
\end{align*}

So in practice, we could run on individual data points, but instead minibatches work well. They're unbiased, but
a subsample:
\begin{enumerate}
  \item Select size $M$ minibatch of data
  \item For $n=1\cdots M$ set $\phi_n=\E_{Q(\lambda)}\left[ \eta(\cdot) \right]$
  \item For $n=1\cdots M$ $\hat{\lambda}_n=\E_{Q(\phi)}\left[ \eta(\cdot) \right]$ with estimate of gradient which is the
  best guess for the parameter based on a single example.
  \item Update $\lambda^{(t)}=(1-\rho_t)\lambda^{(t-1)}+\frac{\rho_t}{M}\sum_{n=1}^M \hat{\lambda}_n$
\end{enumerate}

\begin{align*}
  x_n\sim N(z_n^\top\mu, \sigma^2)
  \\
  \mu\sim N(\mu_0)
  \\
  z_n\sim \textrm{Multinomial}
\end{align*}

So the variational approximation gives $z$ as a Multinomial and $\mu$ as a Normal, both of which are in the exponential
family.

For additional reference (because I could barely keep up):
\begin{description}
  \mycite{miller2016natural}
  \mycite{hoffman2013stochastic}
  ``This is one of the most important papers in Bayesian inference in the last 10 years'' --James.
\end{description}

Advice: do the algebra once for the GMM update, taking for granted the definition of the natural gradient for
exponential families as something to do once.

\section{2017-10-02: Neural Networks and Backpropagation (Yuguang)}
\subsection{Introduction}
Proposed in 1943, based on rough outline of how the brain works.

\subsection{Notation}
\begin{itemize}
  \item $L$ total layers
  \item $S_l$ number of perceptrons in $l$th layer
  \item $w^l_{ji}$ weight from i in layer (l-1) to j in l
  \item $z^l_i$ input of perceptron i in $l$th layer (including bias)
  \item $\sigma$ activation function
  \item $a^l_i$ activation of perceptron i in $l$th layer
  \item $b^l_i$ bias of ---
  \item $C$ cost function: $C=\frac{1}{n}\sum_xC_x$ and $C=\frac12\norm{y-a^L}^2$
  \item $\odot$ Hadamard product
\end{itemize}

\subsection{Forward step}
Initialize weights with random values, this lets us pass forward, then later update
\begin{equation*}
  w_{ji}^{l'}=w_{ji}^l - \alpha \frac{\partial C}{\partial{w_{ji}^l}}
\end{equation*}
and similar for bias terms.

Define $\delta_i^l=\frac{\partial C}{\partial Z_i^l}$ where $Z_i^l=\sum_k w_{ik}^l a^{l-1}_k+b_i^l$
and $\delta^L=\nabla_a C\odot \sigma'(Z^L)$ and for $l<L$, $\delta^l=\left( \left[ W^{L+1} \right]^\top \delta^{l+1} \right) \odot \sigma'(z^l)$

Then $\frac{\partial C}{\partial b_i^L} = \delta_i^L$ and $\frac{\partial C}{\partial w_{ji}^L} = a_i^{l-1}\delta_j^L$

\subsection{Backward step}
Do gradient update for last layer, then work backwards. Use a fancy application of the chain rule, using
calculations we made on the forward step:

\begin{align*}
  \delta_i^L&=\frac{\partial C}{\partial Z_i^L}=\sum_k \frac{\partial C}{\partial a_k^L} \frac{\partial a_k^L}{\partial z_i^L}
  \\
  a^L&=\sigma(z^L)=\sigma(W^L a^{L-1} + b^L)
\end{align*}
So that only $a_i^L$ has a term with $z_i^L$.
So we can simplify:
\begin{align*}
  \delta_i^L&=\frac{\partial C}{\partial Z_i^L}=\frac{\partial C}{\partial a_i^L} \frac{\partial a_i^L}{\partial z_i^L}
  \\
  &= \frac{\partial C}{\partial a_i^L} \sigma'(Z_i^L)
\end{align*}

Do a similar chain rule for the hidden layers, bias terms. Eventually arrive at something that is just a function of
things we computed on the forward pass.

\section{2017-10-23: Classical autoencoders (Mauricio)}
Mostly today, code on GitHub group.

\subsection{Motivation}
Data compression, denoising, hashing, feature extraction, transfer learning.

\subsection{Description}
Input and output are the same, just with a bottleneck layer (or set of layers). First part is encoder, second is
decoder.

Similar in ideology to PCA, trying to find some ``hyperplane'' that explains data simply. PCA maximizes
the variance of a linear combination of principal components. Solution comes from eigenvectors from estimated
covariance matrix of $X$.

Instead, can show that PCA (where $W$ has the eigenvectors) satisfies the reconstruction error problem:
\begin{equation*}
  \min_W\E \left( (WW^\top X_i - X_i)^2 \right)\Rightarrow\min_W \frac{1}{N} \sum_{i=1}^N\norm{WW'x_i-x_i}^2
\end{equation*}

It's like having a single hidden layer where $W_1=W$ and $W_2=W'$, with no activation function (technically, identity).

Could do approximate PCA this way, but not necessarily orthogonal, only reasonable to use SGD on large data, but there
are different PCA algorithms that would be efficient.

Instead, we allow any activation, any size of network, etc. Just have a bottleneck in the middle (that also gets a bias).

Idea from neuroscience (visual system): neurons fire for a specific pattern.


\section{2017-10-30: Variational Autoencoders (Jennifer)}

Data $x$, latent $z$, approximate $p(z|x)$ where $p(x)=\int p(x|z)p(z)dz$ is intractible.

Reminder, minimizing KL divergence is intractable, but maximizing ELBO will minimize it indirectly and tractably.

Going to use encoder $q_\phi(z|x)$, latent $z\sim N(\mu, \sigma I)$, decoder $p_\theta(x|z)$.

Use the reparameterization trick is $z=\mu+\sigma \odot \epsilon$ where $\epsilon\sim N(0,I)$. This lets you do
backpropagation on $\mu$ and $\sigma$. Just do that draw once per training epoch / training. They are the output of the first
neural network (the encoder) such that $(\mu, \sigma)=f(x|\phi)$. At the end, $\tilde{x}=f(z;\theta)$ with $\theta$ as the
output parameters. Can always put a Bernoulli or Normal as the likelihood instead.

Going to use the -ELBO as the loss function we're optimizing to find $\phi$ and $\theta$ (the variational parameters), while
noting the Bayesian (model) parameter $z$.


% If using biblatex
% \printbibliography

\end{document}
